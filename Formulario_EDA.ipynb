{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Proceso: preparación Transaccional + EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar paquetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "letter = string.ascii_uppercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función: Importar Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrar filas y columnas vacías ENTREMEDIO de la tabla importada (agregar dropna después de importar el excel)\n",
    "\n",
    "def importar_excel(nombre_excel, xlsx=True):\n",
    "    \n",
    "    x = pd.ExcelFile(nombre_excel)\n",
    "    y = x.sheet_names\n",
    "    dict = {}\n",
    "    for i, j in zip(range(0,len(y)),y):\n",
    "        dict.update({i : j})\n",
    "    \n",
    "    hoja = int(input(f'¿Cuál de las siguientes hojas quieres importar? (Selecciona el número de la hoja): \\n {dict} \\n '))\n",
    "    \n",
    "    \n",
    "    if xlsx is True:   \n",
    "        nombre_csv = nombre_excel[0:-5]\n",
    "    else: \n",
    "        nombre_csv = nombre_excel[0:-4]\n",
    "        \n",
    "    df1 = pd.read_excel(nombre_excel, sheet_name=hoja)\n",
    "    \n",
    "    def get_first_row(df):\n",
    "        for index, row in df.iterrows():\n",
    "            if not row.isnull().values.all():\n",
    "                if index != 0:\n",
    "                    return index + 1\n",
    "                else:\n",
    "                    return index\n",
    "            \n",
    "    def get_start_column(df):\n",
    "        for i, column in enumerate(df.columns):\n",
    "            if df[column].first_valid_index():\n",
    "                return letter[i]\n",
    "            elif df.iloc[0].first_valid_index():\n",
    "                return 'A'\n",
    "    \n",
    "    def get_last_column(df):\n",
    "        for i, column in enumerate(df.columns):\n",
    "            if get_start_column(df):\n",
    "                if len(df.columns)-i <=25:\n",
    "                    return letter[len(df.columns) - i]\n",
    "                else:\n",
    "                    return f\"A{letter[(len(df.columns) - i)-25]}\"\n",
    "\n",
    "    def usecols(df):\n",
    "        start = get_start_column(df)\n",
    "        end = get_last_column(df)\n",
    "        return f\"{start}:{end}\"\n",
    "\n",
    "    df = pd.read_excel(nombre_excel, sheet_name=hoja, header=get_first_row(df1), usecols=usecols(df1))\n",
    "    archivo_csv = df.to_csv(f\"{nombre_csv}.csv\",\n",
    "                            index = None,\n",
    "                            header = True)\n",
    "    archivo_csv = pd.DataFrame(pd.read_csv(f\"{nombre_csv}.csv\"))\n",
    "    \n",
    "    return archivo_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usar función para importar archivo excel y asignar a dataframe \"orig\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = importar_excel('ruta de acceso archivo excel', xlsx=False);orig.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar archivo .csv y asignar a dataframe \"orig\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = pd.read_csv('ruta de acceso archivo .csv',\n",
    "                   delimiter=',');orig.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función: Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permite ver valores únicos por campo, nulos y no-nulos, formato, máximo y mínimo.\n",
    "# Además un resumen estadístico con los rangos y distribución de la data\n",
    "\n",
    "def overview(dataframe):\n",
    "    \n",
    "    data_resumen = {'Valores Unicos':dataframe.nunique(),'No-Nulos': dataframe.notnull().sum(), \n",
    "                    'Nulos': dataframe.isnull().sum(), 'Formato': dataframe.dtypes, 'Min': dataframe.min(),\n",
    "                    'Max': dataframe.max()}\n",
    "    resumen = pd.DataFrame(data=data_resumen)\n",
    "    print(dataframe.describe())\n",
    "    \n",
    "    return resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usar función para un resumen del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview(orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para unir verticalmente dos dataframes con los mismos campos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para unir verticalmente dos dataframes\n",
    "trs = pd.concat([df1, df2], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para filtrar un dataframe usando otro con las características, y crear nuevo dataframe con filtro aplicado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para unir dos dataframes con columna en comun\n",
    "common = trs.merge(df_filtro,on=['campo_en_comun'])\n",
    "df_filtrado = trs[(df.material.isin(common.campo_en_comun))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para filtrar un dataframe y solo ver un SKU en específico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trs[trs['sku'] == 10001266]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función: Esquema Transaccional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define una lista con los índices del dataframe junto con los nombres de los campos\n",
    "# para armar dataframe nuevo (transaccional)\n",
    "\n",
    "def esquema_transaccional(df,index_campos,_1='fecha', _2='id_cliente', _3='nro_transaccion', \n",
    "                          _4='sku',_5='cantidad', _6='unidadmedida', _7=None, _8=None):\n",
    "\n",
    "    campos = df.columns.values[index_campos].tolist()\n",
    "    columnas = [_1,_2,_3,_4,_5,_6,_7,_8]\n",
    "    transaccional = []\n",
    "\n",
    "    for (campo,columna) in zip(campos,columnas):\n",
    "        df0 = df.rename(columns=dict(zip(campos, columnas)))\n",
    "        transaccional.append(columna)\n",
    "        \n",
    "    df1 = df0[transaccional]\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usar función para armar dataframe transaccional \"trs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trs = esquema_transaccional(orig,[0,1,2,3,4,5]);trs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función: Nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nulos(df):\n",
    "\n",
    "    is_NaN = df.isnull()\n",
    "    row_has_NaN = is_NaN.any(axis=1)\n",
    "    rows_with_NaN = df[row_has_NaN]\n",
    "    n_nulos = len(rows_with_NaN)\n",
    "    \n",
    "    if n_nulos > 0:\n",
    "        \n",
    "        print(f'¡Se han encontrado {n_nulos} registros que contienen valores nulos!\\n')\n",
    "        \n",
    "        dict = {'Analizar': 1, 'Borrar': 0}\n",
    "        answer = int(input(f'¿Qué quieres hacer con estos registros? (Escribe el número de la respuesta): \\n {dict} \\n '))\n",
    "        \n",
    "        if answer == 0:\n",
    "            df.dropna(inplace=True)\n",
    "            return df\n",
    "        \n",
    "        else:\n",
    "            print(rows_with_NaN.nunique())\n",
    "            return rows_with_NaN\n",
    "        \n",
    "    else:\n",
    "        print(f'¡No se encontraron registros nulos!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usar función para revisar registros con celdas nulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulos(trs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función: Duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicados(df):\n",
    "    \n",
    "    duplicateDFRow = df[df.duplicated()]\n",
    "    n_duplicados = len(duplicateDFRow)\n",
    "    \n",
    "    if n_duplicados > 0:\n",
    "\n",
    "        print(f'¡Se han encontrado {n_duplicados} registros duplicados!')\n",
    "        dict = {'Analizar': 1, 'Borrar': 0}\n",
    "        answer = int(input(f'¿Qué quieres hacer con estos registros? (Escribe el número de la respuesta): \\n {dict} \\n '))\n",
    "        \n",
    "        if answer == 0:\n",
    "            df1 = df\n",
    "            df1.drop_duplicates(keep='first',inplace=True)\n",
    "            return df1\n",
    "        \n",
    "        else:\n",
    "            print(duplicateDFRow.nunique())\n",
    "            return duplicateDFRow\n",
    "        \n",
    "    else:\n",
    "        print(f'¡No se encontraron registros duplicados!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usar función para revisar y/o eliminar registros duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicados(trs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función: Columna Fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_fecha(df,columna_fecha, texto=False):\n",
    "    \n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Opción para casos donde la fecha esté separada en columnas de año y mes, para armar columna mensual\n",
    "    if texto == True:\n",
    "        month = []\n",
    "        for i in range (0,len(df)):\n",
    "            mes = df.iloc[i,1]\n",
    "    \n",
    "            if mes == 'ENERO':\n",
    "                mes_i = '01'\n",
    "            elif mes == 'FEBRERO':\n",
    "                mes_i = '02'\n",
    "            elif mes == 'MARZO':\n",
    "                mes_i = '03'\n",
    "            elif mes == 'ABRIL':\n",
    "                mes_i = '04'\n",
    "            elif mes == 'MAYO':\n",
    "                mes_i = '05'\n",
    "            elif mes == 'JUNIO':\n",
    "                mes_i = '06'\n",
    "            elif mes == 'JULIO':\n",
    "                mes_i = '07'\n",
    "            elif mes == 'AGOSTO':\n",
    "                mes_i = '08'\n",
    "            elif mes == 'SEPTIEMBRE':\n",
    "                mes_i = '09'\n",
    "            elif mes == 'OCTUBRE':\n",
    "                mes_i = '10'\n",
    "            elif mes == 'NOVIEMBRE':\n",
    "                mes_i = '11'\n",
    "            elif mes == 'DICIEMBRE':\n",
    "                mes_i = '12'\n",
    "\n",
    "            month.append(mes_i)\n",
    "    \n",
    "        df['mes'] = month\n",
    "        df['año'] = df['año'].apply(str)\n",
    "        \n",
    "        # Se especifica si existe un campo con el día para usarlo en la construcción de la fecha, o se deja \"01\" por defecto\n",
    "        if 'dia' in df.columns:\n",
    "            df['fecha'] = df['año'] + '-' + df['mes'] + df['dia'].apply(str)\n",
    "        else:\n",
    "            df['fecha'] = df['año'] + '-' + df['mes'] + '-01'\n",
    "    \n",
    "    # Deja la nueva columna de fecha en el indice 0 (por motivos de orden de la serie de tiempo)\n",
    "    cols = list(df)\n",
    "    cols.insert(0, cols.pop(cols.index('fecha')))\n",
    "    df = df.loc[:, cols]\n",
    "\n",
    "    df[columna_fecha] = df[columna_fecha].str.slice(stop=10)\n",
    "\n",
    "    date = []\n",
    "    for i in range(0,len(df)):\n",
    "        \n",
    "        fecha_i = df.iloc[i,0]\n",
    "        if fecha_i[2] == '-':\n",
    "            \n",
    "            if fecha_i[-3] == '-':\n",
    "                fecha_i = pd.to_datetime(fecha_i, format = '%d-%m-%y')\n",
    "            else:\n",
    "                fecha_i = pd.to_datetime(fecha_i, format = '%d-%m-%Y')\n",
    "           \n",
    "        else:\n",
    "            fecha_i = pd.to_datetime(fecha_i, format = '%Y-%m-%d')\n",
    "     \n",
    "        month_i = fecha_i.month\n",
    "        year_i = fecha_i.year\n",
    "        day_i = fecha_i.day\n",
    "        fecha_i2 = f'{year_i}-'+f'{month_i}-'+f'{day_i}'\n",
    "            \n",
    "        date.append(fecha_i2)\n",
    "\n",
    "    df[columna_fecha] = date\n",
    "    df[columna_fecha] = pd.to_datetime(df[columna_fecha], format = '%Y-%m-%d')\n",
    "    df.sort_values(by=[columna_fecha], inplace = True, ascending = False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usar función para crear y/o formatear campo de fecha en transaccional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trs = col_fecha(trs,'fecha');trs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función: Columna(s) Entero(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enteros(df,_1='cantidad',_2=None):\n",
    "    \n",
    "    campos = [_1,_2]\n",
    "    \n",
    "    for x in campos:\n",
    "        df[x].fillna(0, inplace = True)\n",
    "        df[x] = df[x].astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usar función para formatear en enteros el/los campo(s) con valores a predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trs = enteros(trs,'cantidad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función: Runners del negocio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runners(df, semana=False): # semanal: 24. mensual: 12\n",
    "\n",
    "    if semana == False:\n",
    "        df1 = df.groupby(['sku',pd.Grouper(key='fecha', freq='MS')]).sum().reset_index()\n",
    "        ventanas = 12\n",
    "    else:    \n",
    "        df1 = df.groupby(['sku',pd.Grouper(key='fecha', freq='W-MON')]).sum().reset_index()\n",
    "        ventanas = 24\n",
    "        \n",
    "    df1.sort_values(by=['fecha'], ascending=False, inplace=True)\n",
    "    df1['distancia'] = df1['fecha'].diff().dt.days.ne(0).cumsum()\n",
    "    df1['peso'] = round((1/df1['distancia']),2)\n",
    "    df1 = df1[df1['distancia'] <= ventanas]\n",
    "    df1 = df1.groupby(['sku']).sum().reset_index()\n",
    "    \n",
    "    thresh = []\n",
    "    for i in range(1, ventanas+1):\n",
    "        x = 1/i\n",
    "        thresh.append(x)\n",
    "        \n",
    "    x = round(sum(thresh),2)\n",
    "    \n",
    "    clasificacion = []\n",
    "    \n",
    "    for i in range(0,len(df1)):\n",
    "        clas = df1.loc[df1.index[i], 'peso']\n",
    "\n",
    "        if clas < x*0.25:\n",
    "            clas_i = 'LR'\n",
    "        elif (clas >= x*0.25 and clas < x*0.75):\n",
    "            clas_i = 'MR'\n",
    "        elif clas >= x*0.75:\n",
    "            clas_i = 'HR'\n",
    "\n",
    "        clasificacion.append(clas_i)\n",
    "\n",
    "    df1['clasificacion'] = clasificacion\n",
    "    print(df1['clasificacion'].value_counts())\n",
    "    \n",
    "    df1['participacion_Q'] = df1['cantidad'] * df1['peso']\n",
    "    df1['participacion_$'] =(df1['ingreso_neto'] * df1['peso']).astype(int)\n",
    "    \n",
    "    df1 = df1[['sku','cantidad','ingreso_neto','clasificacion','participacion_Q','participacion_$']]\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usar función para detectar High, Medium y Low Runners del negocio, para últimos 12 meses o 24 semanas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "runners(trs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función: Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers(df, factor = 2, muestra=10):\n",
    "\n",
    "    df2 = df.groupby(['sku', pd.Grouper(key='fecha', freq='MS')])['cantidad'].sum().reset_index()\n",
    "    df_principal = df2[['fecha', 'sku', 'cantidad']]\n",
    "    df_principal = df2.rename(columns={'cantidad':'total_mes'})\n",
    "    df_principal['mean'] = round(df_principal.groupby(['sku'])['total_mes'].transform('mean'),2)\n",
    "    df_principal['std'] = round(df_principal.groupby(['sku'])['total_mes'].transform('std').fillna(0),2)\n",
    "    \n",
    "    valores = []\n",
    "    df_grafico = pd.DataFrame(np.array([1, 2, 3, 4, 5]), columns=['factor'])\n",
    "    \n",
    "    for i in range(1,6):\n",
    "        \n",
    "        df_principal[f'threshold_{i}'] = (df_principal['mean'] + (i * df_principal['std'])).astype(int)\n",
    "        df_principal[f'diff_{i}'] = (df_principal['total_mes'] - df_principal[f'threshold_{i}']).astype(int)\n",
    "        n_outliers_i = len(df_principal[df_principal[f'diff_{i}'] > 0])\n",
    "        valores.append(n_outliers_i)\n",
    "        \n",
    "    del df_principal['mean'] \n",
    "    del df_principal['std']\n",
    "    \n",
    "    df_grafico['n_outliers'] = valores\n",
    "    print(df_grafico)\n",
    "    \n",
    "    plt.figure(figsize=(13,8))\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    ax = sns.barplot(x=df_grafico['factor'], y=df_grafico['n_outliers'], palette=\"Blues_r\", alpha=0.8)\n",
    "    for index, row in df_grafico.iterrows():\n",
    "        ax.text(row.name,row.n_outliers, int(row.n_outliers), color='black', ha=\"center\")\n",
    "\n",
    "    df_principal.sort_values(by=[f'diff_{factor}'], ascending=False, inplace=True)\n",
    "    df_principal = df_principal[['fecha', 'total_mes',f'threshold_{factor}',f'diff_{factor}']]\n",
    "\n",
    "    return df_principal.head(muestra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usar función para ver nº de outliers según factor, dataframe con factor elegido y gráfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers(df, factor = 2, muestra = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agrupar mensualmente las ventas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trsmes = trs.groupby([pd.Grouper(key='fecha', freq='MS'),'id_cliente','canal','sku']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función: Pareto del negocio (Ingresos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pareto_p(df):\n",
    "    \n",
    "    df1 = df.groupby('sku').sum().reset_index()\n",
    "    del df1['precioVenta']\n",
    "    total_ingresos = df1['ingreso_neto'].sum()\n",
    "    df1['peso_$ (%)'] = round((df1['ingreso_neto'].div(total_ingresos,axis=0))*100,3)\n",
    "    df1.sort_values(by=['peso_$ (%)'], ascending=False, inplace=True)\n",
    "    df1['acumulado'] = df1['peso_$ (%)'].cumsum()\n",
    "    df2 = df1[df1['acumulado'] <= 80.1]\n",
    "    n_sku_pareto = df2['sku'].nunique()\n",
    "    print(f'Número de SKUs que componen el 80% de los ingresos del negocio = {n_sku_pareto}')\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usar función para armar dataframe con el pareto del negocio, asignar a dataframe para ver resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_p = pareto_p(trs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función: Pareto del negocio (Un. vendidas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pareto_q(df):\n",
    "    \n",
    "    df1 = df.groupby('sku').sum().reset_index()\n",
    "    del df1['precioVenta']\n",
    "    total_cantidad = df1['cantidad'].sum()\n",
    "    df1['peso_q (%)'] = round((df1['cantidad'].div(total_cantidad,axis=0))*100,3)\n",
    "    df1.sort_values(by=['peso_q (%)'], ascending=False, inplace=True)\n",
    "    df1['acumulado'] = df1['peso_q (%)'].cumsum()\n",
    "    df2 = df1[df1['acumulado'] <= 80.1]\n",
    "    n_sku_pareto = df2['sku'].nunique()\n",
    "    print(f'Número de SKUs que componen el 80% de las unidades vendidas del negocio = {n_sku_pareto}')\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usar función para armar dataframe con el pareto del negocio, asignar a dataframe para ver resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_q = pareto_q(trs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordena de menor a mayor (True) y viceversa las unidades vendidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trs.sort_values(by=['cantidad'], ascending=True, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficar comportamiento total de ventas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comportamiento total de ventas\n",
    "sns.set(rc={'figure.figsize':(15,10)})\n",
    "sns.lineplot(data=trsmes.groupby(['fecha'])['cantidad'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función: Top 10 por agrupación - Cantidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 por categoria\n",
    "def ventas_top10(df,jerarquia,top=10):\n",
    "    \n",
    "    jer = df.groupby([jerarquia]).sum()\n",
    "    jer = jer.sort_values(by='cantidad',ascending=False)\n",
    "    jer1 = jer.nlargest(top, 'cantidad').reset_index()\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(17,10))\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    ax = sns.barplot(x=jer1[jerarquia], y=jer1['cantidad'], order=jer1[jerarquia],palette=\"Blues_r\",alpha=0.8)\n",
    "    for index, row in jer1.iterrows():\n",
    "        ax.text(row.name,row.cantidad, int(row.cantidad), color='black', ha=\"center\")\n",
    "    \n",
    "    jerarquia = jerarquia.capitalize()\n",
    "    plt.title(f\"Top {top} {jerarquia} - Volumen\", fontsize=14)\n",
    "    plt.ylabel(\"Unidades Vendidas\", fontsize=14)\n",
    "    plt.xlabel(f\"{jerarquia}\", fontsize=14)\n",
    "    \n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usar función para graficar top 10 - Cantidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ventas_top10(trsmes,'jerarquia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función: Top 10 por agrupación - Ingresos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 en ventas (si hay monto facturado)\n",
    "def ingresos(df,jerarquia, top=10):\n",
    "    \n",
    "    jer = df.groupby([jerarquia]).sum()\n",
    "    jer = jer.sort_values(by='ingreso_neto',ascending=False)\n",
    "    jer1 = jer.nlargest(top, 'ingreso_neto').reset_index()\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(17,10))\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    ax = sns.barplot(x=jer1[jerarquia], y=jer1['ingreso_neto'], order=jer1[jerarquia],palette=\"Blues_r\",alpha=0.8)\n",
    "    for index, row in jer1.iterrows():\n",
    "        ax.text(row.name,row.ingreso_neto, int(row.ingreso_neto), color='black', ha=\"center\")\n",
    "    \n",
    "    jerarquia = jerarquia.capitalize()\n",
    "    plt.title(f\"Top {top} {jerarquia} - Ingresos Netos\", fontsize=14)\n",
    "    plt.ylabel(\"Monto en $\", fontsize=14)\n",
    "    plt.xlabel(f\"{jerarquia}\", fontsize=14)\n",
    "    \n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usar función para graficar top 10 - Ingresos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingresos(trsmes,'canal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Función que corre pero toma mucho tiempo, versión de prueba\n",
    "\n",
    "def outliers1(sku='sku', fecha='fecha', cantidad='real', factores=[1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5]):\n",
    "    \n",
    "    # Creación dataframe con totales por sku por fecha (al final se agregan thresholds a cada uno)\n",
    "    \n",
    "    data_df_principal = {'sku': df1[sku],'fecha_mes':df1[fecha], 'real': df1[cantidad]}\n",
    "    dfp = pd.DataFrame(data=data_df_principal)\n",
    "    df_principal = dfp.groupby(['sku', pd.Grouper(key='fecha_mes', freq='MS')]).sum()\n",
    "    df_principal.reset_index(drop=False, inplace=True)\n",
    "    \n",
    "    # Construcción thresholds\n",
    "\n",
    "    factores_prueba = factores\n",
    "    n_outliers = []\n",
    "    for factor in factores_prueba:\n",
    "        \n",
    "        df_thresholds = df_principal.groupby(['sku']).mean()\n",
    "        df_thresholds['std'] = df_principal.groupby(['sku']).std()\n",
    "        df_thresholds = df_thresholds.rename(columns={'real': 'mean'})\n",
    "        df_thresholds['std'] = df_thresholds['std'].fillna(0)\n",
    "        df_thresholds['threshold'] = df_thresholds['mean'] + (factor*df_thresholds['std'])\n",
    "        df_thresholds.reset_index(drop=False, inplace=True)\n",
    "\n",
    "        # Tomar thresholds y armar la columna para df_principal, para comparar\n",
    "\n",
    "        valores = []\n",
    "\n",
    "        for k in range(0, len(df_thresholds)):\n",
    "            threshold_valor = df_thresholds.iloc[k,3]\n",
    "\n",
    "            for i in df_principal['sku']:\n",
    "                o = df_thresholds.iloc[k,0]            \n",
    "\n",
    "                if i == o:\n",
    "                    valores.append(threshold_valor)\n",
    "\n",
    "        df_principal['threshold'] = valores\n",
    "\n",
    "        outliers = []\n",
    "        diff = []\n",
    "        for j in range(0, len(df_principal)):\n",
    "            real = df_principal.iloc[j,2]\n",
    "            thresh = df_principal.iloc[j,3]\n",
    "            dif = abs(real - thresh)\n",
    "\n",
    "            diff.append(dif)\n",
    "\n",
    "            if real > thresh:\n",
    "                outliers.append(1)\n",
    "            else:\n",
    "                outliers.append(0)\n",
    "\n",
    "\n",
    "        df_principal['outliers'] = outliers\n",
    "        df_principal['outliers'] = df_principal['outliers'].astype(int)\n",
    "        df_principal['diff'] = diff\n",
    "        df_principal['diff'] = df_principal['diff'].astype(int)\n",
    "        df_output = df_principal[df_principal['outliers'] == 1]\n",
    "        del df_output['outliers']\n",
    "        df_output.sort_values(by=['diff'], ascending=False)\n",
    "        n_outliers.append(len(df_output))\n",
    "        \n",
    "        df_principal = dfp.groupby(['sku', pd.Grouper(key='fecha_mes', freq='MS')]).sum()\n",
    "        df_principal.reset_index(drop=False, inplace=True)\n",
    "        \n",
    "    data_df_outliers = {'factor': factores_prueba,'n_outliers': n_outliers}\n",
    "    df_outliers = pd.DataFrame(data=data_df_outliers)\n",
    "    print(df_outliers)\n",
    "    plt.plot(df_outliers['factor'],df_outliers['n_outliers'], color = 'blue', marker = 'o')\n",
    "    plt.title('Outliers según Factor')\n",
    "    plt.xlabel('Factor')\n",
    "    plt.ylabel('Nº Outliers')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    return df_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Al terminar de procesar transaccional, exportar dataframe a archivo .csv (UTF-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para exportar dataframe de transaccional a un csv\n",
    "trs.to_csv(r'/ruta_de_archivo/titulo_archivo_transaccional.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Proceso: Transaccional + Inferencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función: Métricas Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para calcular smape individual por ventana de tiempo + otros cálculos generales, uniendo archivo de inferencias con transaccional\n",
    "\n",
    "def metricas_error(df,inferencia='inference.csv', semanal = False, agrupacion=None):\n",
    "\n",
    "    # importar archivo inferencias\n",
    "    inferencia = pd.read_csv(inferencia)\n",
    "    inferencia.sort_values(by=['item','date'], ascending=True, inplace=True)\n",
    "    inferencia['date'] = pd.to_datetime(inferencia['date'], format = '%Y-%m-%d')\n",
    "\n",
    "    # adaptar transaccional\n",
    "\n",
    "    # Caso Mensual\n",
    "    if semanal == False:\n",
    "        \n",
    "        # Agrupacion solo por sku\n",
    "        if agrupacion == None:\n",
    "            \n",
    "            transaccional = df.groupby([pd.Grouper(key='fecha', freq='MS'), 'sku']).sum()\n",
    "            transaccional.reset_index(drop=False, inplace=True)\n",
    "            transaccional.sort_values(by=['sku','fecha'], ascending=True, inplace=True)\n",
    "        \n",
    "        # Agrupacion además de sku\n",
    "        else:\n",
    "            transaccional = df.groupby([pd.Grouper(key='fecha', freq='MS'), 'sku']).sum()\n",
    "            transaccional.reset_index(drop=False, inplace=True)\n",
    "            transaccional.sort_values(by=[agrupacion,'sku','fecha'], ascending=True, inplace=True)\n",
    "            transaccional[\"item\"] = transaccional[agrupacion].astype(str) + '|' + transaccional[\"sku\"].astype(str)\n",
    "            del transaccional['sku']\n",
    "            del transaccional[agrupacion]\n",
    "        \n",
    "    # Caso semanal\n",
    "    else:\n",
    "        \n",
    "        # Agrupacion solo por sku\n",
    "        if agrupacion == None:\n",
    "            \n",
    "            transaccional = df.groupby([pd.Grouper(key='fecha', freq='W-MON'), 'sku']).sum()\n",
    "            transaccional.reset_index(drop=False, inplace=True)\n",
    "            transaccional.sort_values(by=['sku','fecha'], ascending=True, inplace=True)\n",
    "        \n",
    "        # Agrupacion además de sku\n",
    "        else:\n",
    "            transaccional = df.groupby([pd.Grouper(key='fecha', freq='W-MON'), agrupacion,'sku']).sum()\n",
    "            transaccional.reset_index(drop=False, inplace=True)\n",
    "            transaccional.sort_values(by=[agrupacion,'sku','fecha'], ascending=True, inplace=True)\n",
    "            transaccional[\"item\"] = transaccional[agrupacion].astype(str) + '|' + transaccional[\"sku\"].astype(str)\n",
    "            del transaccional['sku']\n",
    "            del transaccional[agrupacion]\n",
    "        \n",
    "    # Cambio de nombre de columnas y reordenamiento para coincidir con dataframe de inferencias   \n",
    "    transaccional.rename(columns={'sku' : 'item', 'fecha': 'date'}, inplace= True)\n",
    "    transaccional = transaccional[['date', 'item', 'cantidad']]\n",
    "    \n",
    "    # unión transaccional con inferencias\n",
    "    merged=pd.merge(transaccional,inferencia, how='outer')\n",
    "    merged.sort_values(by=['item','date'], ascending=True, inplace=True)\n",
    "    del merged['model']\n",
    "    merged = merged.rename(columns={'cantidad' : 'hist', 'units': 'pred'}).fillna(0)\n",
    "    merged = merged[(merged.date <= max(inferencia['date'])) & (merged.date >= min(inferencia['date']))]\n",
    "    \n",
    "    # construcción de métricas\n",
    "    smape = []\n",
    "    accuracy = []\n",
    "    mape = []\n",
    "    bias = []\n",
    "    hist_pred = []\n",
    "    hist_pred2 = []\n",
    "    \n",
    "    for i in range(0,len(merged)):\n",
    "        hist = merged.iloc[i,2].astype(int)\n",
    "    #    hist = hist + 0.01\n",
    "        pred = merged.iloc[i,3]\n",
    "\n",
    "        hist_pred_i = ((hist) - pred)\n",
    "        hist_pred_i2 = ((hist) - pred)**2\n",
    "        hist_pred.append(hist_pred_i)\n",
    "        hist_pred2.append(hist_pred_i2)\n",
    "\n",
    "        if pred != 0:\n",
    "            bias_i = round((((hist)/pred)-1),5)*100\n",
    "        else:\n",
    "            bias_i = 0\n",
    "        \n",
    "        bias.append(bias_i)\n",
    "\n",
    "        if abs(hist_pred_i) > 0.05:\n",
    "            smape_i = 100*((abs(hist_pred_i)/((abs(hist) + abs(pred))/2)))\n",
    "        else:\n",
    "            smape_i = 0\n",
    "            \n",
    "        smape.append(smape_i)\n",
    "            \n",
    "        if hist != 0:\n",
    "            #mape_i = abs(round((((hist_pred_i)/(hist)),2)))\n",
    "            accuracy_i = (1-(abs(hist_pred_i)/(hist)))*100\n",
    "        else:\n",
    "            #mape_i = 0\n",
    "            accuracy_i = 0\n",
    "        \n",
    "       # mape.append(mape_i)\n",
    "        accuracy.append(accuracy_i)\n",
    "           \n",
    "            \n",
    "    # smape y mape\n",
    "    merged['smape1'] = smape\n",
    "    #merged['mape1'] = mape\n",
    "    #merged['mape1'] = merged['mape1']\n",
    "    \n",
    "    # forecast accuracy\n",
    "    merged['accuracy1'] = accuracy\n",
    "\n",
    "    # bias\n",
    "    merged['bias1'] = bias\n",
    "    \n",
    "    # diferencia absoluta entre historico e inferencia\n",
    "    merged['hist_pred'] = hist_pred\n",
    "    merged['rmse1'] = abs(merged['hist_pred'])\n",
    "    merged['hist_pred2'] = hist_pred2\n",
    "    del merged['hist_pred']\n",
    "    \n",
    "    merged = merged[['date', 'item', 'hist','pred','rmse','rmse1','smape','smape1','accuracy','accuracy1','bias','bias1','hist_pred2']]\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usar función para unir transaccional con inferencias + cálculo métricas, asignar a dataframe \"inf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf = metricas_error(trs,\n",
    "                     'ruta de acceso archivo inferencia',\n",
    "                     semanal=False,\n",
    "                     ag2='tienda');inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide el dataframe \"inf\" y asigna el periodo de validación al dataframe \"val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = inf[inf['date'] <= 'presente-virtual-prediccion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide el dataframe \"inf\" y asigna el periodo de predicción al dataframe \"pron\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pron = inf[inf['date'] > 'presente-virtual-prediccion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función: Sobrepronósticos y Subpronósticos en dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se asigna datafame e índice de la columna con \"bias1\" para revisar cuántos items fueron sobre o sub pronosticados\n",
    "\n",
    "quiebre = []\n",
    "exceso = []\n",
    "\n",
    "for i in range(0,len(pron)):\n",
    "    bias_i = pron.iloc[i,11]\n",
    "    \n",
    "    if bias_i > 0:\n",
    "        quiebre.append(bias_i)\n",
    "    else:\n",
    "        exceso.append(bias_i)\n",
    "\n",
    "nq = len(quiebre)\n",
    "ne = len(exceso)\n",
    "\n",
    "print(f'Numero de items sobrepronosticados: {ne}\\n'\n",
    "     f'Numero de items subpronosticados: {nq}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usar función para mostrar total de predicciones por arriba y debajo, junto con %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nivel_stock(inf,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función: Resumen Inferencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para calcular a nivel de sku o ventana de tiempo smape promedio, bias y forecast accuracy\n",
    "\n",
    "def resumen_inferencias(df=df2, grupo='item'):\n",
    "    \n",
    "    #resultados = df.where(df['pred_hist'] != 0).groupby([grupo]).agg({'hist':'sum','pred':'sum','pred_hist':'sum','pred_hist2':np.mean}).reset_index()\n",
    "    resultados = df.groupby([grupo]).agg({'hist':'sum','pred':'sum','rmse':np.mean,'rmse1':'sum','accuracy':np.mean,'hist_pred2':np.mean}).reset_index()\n",
    "    #resultados['smape'] = df.groupby([grupo]).agg({'smape':np.mean}).reset_index().drop([grupo], axis=1).fillna(0)\n",
    "    resultados['smape'] = round(df.groupby([grupo]).agg({'smape':np.mean}),2).reset_index().drop([grupo], axis=1)\n",
    "    resultados['smape1'] = round(df.groupby([grupo]).agg({'smape1':np.mean}),2).reset_index().drop([grupo], axis=1)\n",
    "    #resultados['mape1'] = round(df.groupby([grupo]).agg({'mape1':np.mean}).reset_index().drop([grupo], axis=1),3).fillna(0)\n",
    "   # resultados['mape_0'] = round(df.where(df['mape1'] != 0).groupby([grupo]).agg({'mape1':np.mean}).reset_index().drop([grupo], axis=1),3).fillna(0)\n",
    "   # resultados['weighted_mape'] = round(((resultados['pred_hist'] / resultados['hist']) - 1),2).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    #resultados['rmse'] = df.groupby([grupo]).agg({'rmse':np.mean}).reset_index().drop([grupo], axis=1)\n",
    "    resultados['hist_pred_abs'] = resultados['rmse1']\n",
    "    resultados['rmse1'] = round((resultados['hist_pred2']**0.5),2)\n",
    "    #resultados['accuracy'] = df.groupby([grupo]).agg({'accuracy':np.mean}).reset_index().drop([grupo], axis=1)\n",
    "    #resultados['accuracy'] = round((1 - (resultados['rmse1'] / resultados['hist'])),2).replace([np.inf, -np.inf], 4)\n",
    "    resultados['accuracy1'] = round((1 - (resultados['hist_pred_abs'] / resultados['hist'])),2).replace([np.inf, -np.inf], 4)\n",
    "    #resultados['bias'] = df.groupby([grupo]).agg({'bias':np.mean}).reset_index().drop([grupo], axis=1)\n",
    "    resultados['bias'] = round(((resultados['hist'] / resultados['pred']) - 1),2)\n",
    "    resultados['hist_pred2']\n",
    "    \n",
    "    resultados = resultados[[grupo, 'hist','pred','hist_pred_abs','rmse','rmse1','smape','smape1','accuracy','accuracy1','bias','hist_pred2']]\n",
    "\n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usar función para calcular métricas a nivel de agrupación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = resumen_inferencias(val);item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resumen_inferencias' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-872137203f72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmes_o_semana\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresumen_inferencias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrupo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mmes_o_semana\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'resumen_inferencias' is not defined"
     ]
    }
   ],
   "source": [
    "mes_o_semana = resumen_inferencias(val,grupo='date');mes_o_semana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función: Métricas Globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metricas_globales(df):\n",
    "    sum_hist = df['hist'].sum()\n",
    "    sum_pred = df['pred'].sum()\n",
    "    sum_hist_pred = df['hist_pred_abs'].sum()\n",
    "    mean_hist_pred2 = df['hist_pred2'].mean()\n",
    "    hist_pred2 = []\n",
    "    \n",
    "    for i in range(0,len(df)):\n",
    "        hist = df.iloc[i,1]\n",
    "        pred = df.iloc[i,2]\n",
    "        \n",
    "        dif = (hist - pred)**2\n",
    "        hist_pred2.append(dif)\n",
    "    \n",
    "    rmse = round((sum(hist_pred2) / len(hist_pred2))**0.5,2)\n",
    "    rmse1 = round(df['rmse'].mean(),2)\n",
    "    smape = round(df['smape'].mean(),3)\n",
    "    accuracy = round(1 - (sum_hist_pred/sum_hist),3)*100\n",
    "    bias = round((sum_hist/sum_pred) - 1,3)\n",
    "    \n",
    "    print(f'RMSE = {rmse} ; {rmse1}\\n'\n",
    "         f'SMAPE = {smape}\\n'\n",
    "         f'FA = {accuracy}%\\n'\n",
    "         f'Bias = {bias}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usar función para calcular métricas a nivel de modelo (globales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricas_globales(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficar SMAPE para periodo validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafico smape\n",
    "sns.set(rc={'figure.figsize':(15,10)})\n",
    "sns.histplot(data=val['smape1'], kde=True,binwidth=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficar SMAPE para periodo pronósticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafico smape 2\n",
    "sns.histplot(data=pron['smape1'], kde=True,binwidth=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficar Forecast Accuracy por ventana de tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafico forecast accuracy\n",
    "sns.set(rc={'figure.figsize':(15,10)})\n",
    "sns.lineplot(data=mes.drop(columns=['rmse','smape','bias','hist_pred2','hist_pred_abs','hist','pred'], axis=1).set_index('date'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficar ventas vs predicciones por ventana de tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafico venta vs prediccion\n",
    "sns.set(rc={'figure.figsize':(15,10)})\n",
    "sns.lineplot(data=mes.drop(columns=['rmse','smape','accuracy','bias','hist_pred2','hist_pred_abs'], axis=1).set_index('date'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficar venta promedio vs predicción promedio por ventana de tiempo + intervalos 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafico venta promedio vs inferencia promedio + intervalos 95%\n",
    "sns.lineplot(data=inf.drop(columns=['rmse','smape','accuracy','bias','hist_pred2','rmse1','smape1','accuracy1','item','bias','bias1'], axis=1).set_index('date'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficar RMSE vs SMAPE vs Volumen ventas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafico rmse vs smape vs volumen\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1 = item[item['rmse1'] <=5000].plot.scatter(x='smape1', y='rmse', c='hist', colormap='plasma',figsize=(20,15), legend = True,logy=True, ax=ax1)\n",
    "ax1.set_xlabel(\"smape\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
